{
  "format_version": "1.0",
  "description": "VERIFIED hardware profiles. Only tested/confirmed combinations.",
  "profiles": {
    "ada_cu124": {
      "description": "NVIDIA Ada Lovelace Architecture - RTX 4000 series",
      "hardware": {
        "compute_capability": [
          "8.9"
        ],
        "cuda_versions": [
          "12.4",
          "12.5",
          "12.6"
        ],
        "gpu_examples": [
          "RTX 4090",
          "RTX 4080",
          "RTX 4070 Ti",
          "RTX 4070",
          "RTX 4060 Ti",
          "RTX 4060"
        ]
      },
      "packages": {
        "typing-extensions": "4.12.2",
        "torch": "2.5.1+cu124",
        "torchvision": "0.20.1+cu124",
        "torchaudio": "2.5.1+cu124",
        "transformers": ">=4.51.0,<4.60.0",
        "safetensors": ">=0.7.0,<0.8.0",
        "huggingface-hub": ">=0.27.0,<0.28.0",
        "accelerate": ">=1.2.0,<1.3.0",
        "datasets": ">=3.2.0,<4.0.0",
        "PySide6": "6.8.1",
        "PySide6-Essentials": "6.8.1",
        "PySide6-Addons": "6.8.1"
      },
      "tested": false,
      "tested_on": [],
      "notes": "Configuration for RTX 4000 series. Uses CUDA 12.4 for optimal performance. Updated to transformers 4.51.3 for peft compatibility.",
      "binary_packages": {}
    },
    "ampere_cu121": {
      "description": "NVIDIA Ampere Architecture - RTX 3000 series + A-series workstations",
      "hardware": {
        "compute_capability": [
          "8.0",
          "8.6"
        ],
        "cuda_versions": [
          "12.1",
          "12.2",
          "12.3"
        ],
        "gpu_examples": [
          "RTX 3090",
          "RTX 3080",
          "RTX 3070",
          "RTX 3060",
          "A100",
          "A30",
          "A2000",
          "A6000"
        ]
      },
      "packages": {
        "typing-extensions": "4.12.2",
        "torch": "2.5.1+cu121",
        "torchvision": "0.20.1+cu121",
        "torchaudio": "2.5.1+cu121",
        "transformers": ">=4.51.0,<4.60.0",
        "safetensors": ">=0.7.0,<0.8.0",
        "huggingface-hub": ">=0.27.0,<0.28.0",
        "accelerate": ">=1.2.0,<1.3.0",
        "datasets": ">=3.2.0,<4.0.0",
        "PySide6": "6.8.1",
        "PySide6-Essentials": "6.8.1",
        "PySide6-Addons": "6.8.1"
      },
      "tested": true,
      "tested_on": [
        "RTX A2000",
        "RTX 4090"
      ],
      "notes": "Verified on RTX A2000 workstation. Compatible with RTX 3000 series and Ampere A-series GPUs.",
      "binary_packages": {}
    },
    "blackwell_cu124": {
      "description": "NVIDIA Blackwell Architecture - RTX 5000 series",
      "hardware": {
        "compute_capability": [
          "10.0"
        ],
        "cuda_versions": [
          "12.4",
          "12.5",
          "12.6"
        ],
        "gpu_examples": [
          "RTX 5090",
          "RTX 5080",
          "RTX 5070 Ti",
          "RTX 5070"
        ]
      },
      "packages": {
        "typing-extensions": "4.12.2",
        "torch": "2.5.1+cu124",
        "torchvision": "0.20.1+cu124",
        "torchaudio": "2.5.1+cu124",
        "transformers": ">=4.51.0,<4.60.0",
        "safetensors": ">=0.7.0,<0.8.0",
        "huggingface-hub": ">=0.27.0,<0.28.0",
        "accelerate": ">=1.2.0,<1.3.0",
        "datasets": ">=3.2.0,<4.0.0",
        "PySide6": "6.8.1",
        "PySide6-Essentials": "6.8.1",
        "PySide6-Addons": "6.8.1"
      },
      "tested": false,
      "tested_on": [],
      "notes": "Configuration for next-gen RTX 5000 series (Blackwell architecture). Requires CUDA 12.4+. Updated to transformers 4.51.3 for peft compatibility.",
      "binary_packages": {}
    },
    "hopper_cu124": {
      "description": "NVIDIA Hopper Architecture - H100 datacenter GPUs",
      "hardware": {
        "compute_capability": [
          "9.0"
        ],
        "cuda_versions": [
          "12.4",
          "12.5",
          "12.6"
        ],
        "gpu_examples": [
          "H100",
          "H100 PCIe",
          "H100 SXM5"
        ]
      },
      "packages": {
        "typing-extensions": "4.12.2",
        "torch": "2.5.1+cu124",
        "torchvision": "0.20.1+cu124",
        "torchaudio": "2.5.1+cu124",
        "transformers": ">=4.51.0,<4.60.0",
        "safetensors": ">=0.7.0,<0.8.0",
        "huggingface-hub": ">=0.27.0,<0.28.0",
        "accelerate": ">=1.2.0,<1.3.0",
        "datasets": ">=3.2.0,<4.0.0",
        "PySide6": "6.8.1",
        "PySide6-Essentials": "6.8.1",
        "PySide6-Addons": "6.8.1"
      },
      "tested": false,
      "tested_on": [],
      "notes": "Configuration for H100 datacenter GPUs. Optimized for large-scale training workloads. Updated to transformers 4.51.3 for peft compatibility.",
      "binary_packages": {}
    },
    "turing_cu118": {
      "description": "NVIDIA Turing Architecture - RTX 2000 series + Quadro T-series",
      "hardware": {
        "compute_capability": [
          "7.5"
        ],
        "cuda_versions": [
          "11.8"
        ],
        "gpu_examples": [
          "RTX 2080 Ti",
          "RTX 2080",
          "RTX 2070",
          "RTX 2060",
          "T1000",
          "T600",
          "T400"
        ]
      },
      "packages": {
        "typing-extensions": "4.8.0",
        "torch": "2.5.1+cu118",
        "torchvision": "0.20.1+cu118",
        "torchaudio": "2.5.1+cu118",
        "transformers": ">=4.45.0,<4.60.0",
        "safetensors": ">=0.4.0,<0.8.0",
        "huggingface-hub": ">=0.25.0,<0.28.0",
        "accelerate": ">=1.0.0,<1.3.0",
        "datasets": ">=3.0.0,<4.0.0",
        "PySide6": "6.7.3",
        "PySide6-Essentials": "6.7.3",
        "PySide6-Addons": "6.7.3"
      },
      "tested": "in_progress",
      "tested_on": [
        "T1000"
      ],
      "notes": "Configuration for T1000 workstation and RTX 2000 series. Uses older package versions for compatibility with CUDA 11.8.",
      "binary_packages": {}
    }
  },
  "fallback_rules": {
    "unknown_newer_gpu": {
      "action": "use_profile",
      "profile": "cuda124_ampere_ada_blackwell",
      "reason": "Newer GPU - try latest CUDA profile"
    },
    "unknown_older_gpu": {
      "action": "use_profile",
      "profile": "cuda118_turing",
      "reason": "Older GPU - use conservative CUDA 11.8"
    },
    "no_cuda": {
      "action": "error",
      "message": "CPU-only mode not yet implemented. NVIDIA GPU required."
    },
    "low_vram": {
      "threshold_gb": 8,
      "action": "warn",
      "message": "GPU has less than 8GB VRAM. Fine-tuning large models may fail or require smaller batch sizes."
    },
    "very_low_vram": {
      "threshold_gb": 4,
      "action": "block",
      "message": "GPU has less than 4GB VRAM. This is insufficient for LLM fine-tuning. Minimum 6GB recommended."
    },
    "old_cuda": {
      "cuda_max": "11.7",
      "action": "warn",
      "message": "CUDA version is older than 11.8. Please update NVIDIA drivers for better compatibility."
    },
    "mixed_gpus": {
      "action": "use_lowest_common",
      "message": "Multiple GPUs detected with different capabilities. Using configuration for the least capable GPU to ensure compatibility."
    }
  },
  "compute_capability_map": {
    "8.9": {
      "architecture": "Ada",
      "profile": "ada_cu124"
    },
    "10.0": {
      "architecture": "Blackwell",
      "profile": "blackwell_cu124"
    },
    "9.0": {
      "architecture": "Hopper",
      "profile": "hopper_cu124"
    },
    "8.0": {
      "architecture": "Ampere",
      "profile": "ampere_cu121"
    },
    "8.6": {
      "architecture": "Ampere",
      "profile": "ampere_cu121"
    },
    "7.5": {
      "architecture": "Turing",
      "profile": "turing_cu118"
    }
  },
  "common_packages": {
    "numpy": "1.26.4",
    "tokenizers": ">=0.21.0,<0.24.0",
    "peft": ">=0.13.0,<0.16.0",
    "bitsandbytes": ">=0.45.0,<0.50.0",
    "sympy": "1.13.1",
    "jinja2": "3.1.6",
    "fsspec": "2025.9.0",
    "filelock": "3.20.1",
    "mpmath": "1.3.0",
    "regex": "2024.11.6",
    "pyyaml": "6.0.2",
    "requests": "2.32.3",
    "packaging": "24.2",
    "tqdm": "4.67.1",
    "sentencepiece": "0.2.0",
    "evaluate": "0.4.3",
    "einops": "0.8.1",
    "timm": "1.0.12",
    "open-clip-torch": "2.29.0",
    "pandas": "2.2.3",
    "psutil": "7.2.0",
    "streamlit": "1.41.1",
    "uvicorn": ">=0.30.0,<1.0.0",
    "fastapi": ">=0.115.0,<1.0.0",
    "pydantic": ">=2.0.0,<3.0.0"
  },
  "blacklist": {
    "packages": [
      "torchao",
      "pytorch-ao",
      "torchao-nightly",
      "torch-ao"
    ],
    "reason": "Cause transformers import failures"
  },
  "unsupported": {
    "gtx_10xx_pascal": "GTX 1080/1070/1060 (compute 6.1) support being dropped in PyTorch 2.8+. May work with CUDA 11.8 profile but untested.",
    "gtx_9xx_maxwell": "GTX 980/970 (compute 5.x) not supported by PyTorch 2.5+",
    "amd_gpu": "AMD GPUs (ROCm) not supported. This installer is NVIDIA-only.",
    "cpu_only": "CPU-only mode not implemented yet."
  }
}
