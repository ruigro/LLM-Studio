{
  "format_version": "1.0",
  "description": "VERIFIED hardware profiles. Only tested/confirmed combinations.",
  
  "profiles": {
    "cuda124_ampere_ada_blackwell": {
      "description": "CUDA 12.4+, RTX 5090/5080/4090/4080/3090/3080 (Blackwell/Ada/Ampere)",
      "hardware": {
        "cuda_versions": ["12.4", "12.5", "12.6"],
        "compute_capability": ["8.6", "8.9", "9.0", "10.0"],
        "gpu_examples": ["RTX 5090", "RTX 5080", "RTX 5070", "RTX 4090", "RTX 4080", "RTX 4070", "RTX 3090", "RTX 3080", "RTX 3070", "RTX 3060"]
      },
      "packages": {
        "typing-extensions": "4.12.2",
        "numpy": "1.26.4",
        "torch": "2.5.1+cu124",
        "torchvision": "0.20.1+cu124",
        "torchaudio": "2.5.1+cu124",
        "transformers": "4.51.3",
        "tokenizers": "0.21.0",
        "safetensors": "0.7.0",
        "huggingface-hub": "0.27.1",
        "peft": "0.13.2",
        "accelerate": "1.2.1",
        "bitsandbytes": "0.44.1",
        "datasets": "3.2.0",
        "PySide6": "6.8.1",
        "PySide6-Essentials": "6.8.1",
        "PySide6-Addons": "6.8.1"
      },
      "tested": true,
      "notes": "Verified on RTX 4090 + RTX A2000"
    },
    
    "cuda121_ampere": {
      "description": "CUDA 12.1-12.3, RTX 3000 series + A-series workstation",
      "hardware": {
        "cuda_versions": ["12.1", "12.2", "12.3"],
        "compute_capability": ["8.0", "8.6"],
        "gpu_examples": ["RTX 3090", "RTX 3080", "RTX 3070", "RTX 3060", "A100", "A30", "A2000", "A6000"]
      },
      "packages": {
        "typing-extensions": "4.12.2",
        "numpy": "1.26.4",
        "torch": "2.5.1+cu121",
        "torchvision": "0.20.1+cu121",
        "torchaudio": "2.5.1+cu121",
        "transformers": "4.51.3",
        "tokenizers": "0.21.0",
        "safetensors": "0.7.0",
        "huggingface-hub": "0.27.1",
        "peft": "0.13.2",
        "accelerate": "1.2.1",
        "bitsandbytes": "0.44.1",
        "datasets": "3.2.0",
        "PySide6": "6.8.1",
        "PySide6-Essentials": "6.8.1",
        "PySide6-Addons": "6.8.1"
      },
      "tested": true,
      "notes": "Verified on RTX A2000"
    },
    
    "cuda118_turing": {
      "description": "CUDA 11.8, RTX 2000 series + Quadro T-series",
      "hardware": {
        "cuda_versions": ["11.8"],
        "compute_capability": ["7.5"],
        "gpu_examples": ["RTX 2080 Ti", "RTX 2080", "RTX 2070", "RTX 2060", "T1000", "T600", "T400"]
      },
      "packages": {
        "typing-extensions": "4.8.0",
        "numpy": "1.26.4",
        "torch": "2.5.1+cu118",
        "torchvision": "0.20.1+cu118",
        "torchaudio": "2.5.1+cu118",
        "transformers": "4.45.2",
        "tokenizers": "0.21.0",
        "safetensors": "0.4.5",
        "huggingface-hub": "0.25.2",
        "peft": "0.12.0",
        "accelerate": "1.0.1",
        "bitsandbytes": "0.43.3",
        "datasets": "3.0.1",
        "PySide6": "6.7.3",
        "PySide6-Essentials": "6.7.3",
        "PySide6-Addons": "6.7.3"
      },
      "tested": "in_progress",
      "notes": "Testing on T1000"
    }
  },
  
  "fallback_rules": {
    "unknown_newer_gpu": {
      "action": "use_profile",
      "profile": "cuda124_ampere_ada_blackwell",
      "reason": "Newer GPU - try latest CUDA profile"
    },
    "unknown_older_gpu": {
      "action": "use_profile",
      "profile": "cuda118_turing",
      "reason": "Older GPU - use conservative CUDA 11.8"
    },
    "no_cuda": {
      "action": "error",
      "message": "CPU-only mode not yet implemented. NVIDIA GPU required."
    },
    "low_vram": {
      "threshold_gb": 8,
      "action": "warn",
      "message": "GPU has less than 8GB VRAM. Fine-tuning large models may fail or require smaller batch sizes."
    },
    "very_low_vram": {
      "threshold_gb": 4,
      "action": "block",
      "message": "GPU has less than 4GB VRAM. This is insufficient for LLM fine-tuning. Minimum 6GB recommended."
    },
    "old_cuda": {
      "cuda_max": "11.7",
      "action": "warn",
      "message": "CUDA version is older than 11.8. Please update NVIDIA drivers for better compatibility."
    },
    "mixed_gpus": {
      "action": "use_lowest_common",
      "message": "Multiple GPUs detected with different capabilities. Using configuration for the least capable GPU to ensure compatibility."
    }
  },
  
  "compute_capability_map": {
    "10.0": {"architecture": "Blackwell", "profile": "cuda124_ampere_ada_blackwell"},
    "9.0": {"architecture": "Hopper", "profile": "cuda124_ampere_ada_blackwell"},
    "8.9": {"architecture": "Ada Lovelace", "profile": "cuda124_ampere_ada_blackwell"},
    "8.6": {"architecture": "Ampere (RTX 30xx)", "profile": "cuda121_ampere"},
    "8.0": {"architecture": "Ampere (A-series)", "profile": "cuda121_ampere"},
    "7.5": {"architecture": "Turing (RTX 20xx)", "profile": "cuda118_turing"}
  },
  
  "common_packages": {
    "sympy": "1.13.1",
    "jinja2": "3.1.6",
    "fsspec": "2025.9.0",
    "filelock": "3.20.1",
    "mpmath": "1.3.0",
    "regex": "2024.11.6",
    "pyyaml": "6.0.2",
    "requests": "2.32.3",
    "packaging": "24.2",
    "tqdm": "4.67.1",
    "sentencepiece": "0.2.0",
    "evaluate": "0.4.3",
    "einops": "0.8.1",
    "timm": "1.0.12",
    "open-clip-torch": "2.29.0",
    "pandas": "2.2.3",
    "psutil": "7.2.0",
    "streamlit": "1.41.1"
  },
  
  "blacklist": {
    "packages": ["torchao", "pytorch-ao", "torchao-nightly", "torch-ao"],
    "reason": "Cause transformers import failures"
  },
  
  "unsupported": {
    "gtx_10xx_pascal": "GTX 1080/1070/1060 (compute 6.1) support being dropped in PyTorch 2.8+. May work with CUDA 11.8 profile but untested.",
    "gtx_9xx_maxwell": "GTX 980/970 (compute 5.x) not supported by PyTorch 2.5+",
    "amd_gpu": "AMD GPUs (ROCm) not supported. This installer is NVIDIA-only.",
    "cpu_only": "CPU-only mode not implemented yet."
  }
}
