{
  "format_version": "1.0",
  "description": "VERIFIED hardware profiles. Only tested/confirmed combinations.",
  
  "profiles": {
    "cuda124_ampere_ada_blackwell": {
      "description": "CUDA 12.4+, RTX 5090/5080/4090/4080/3090/3080 (Blackwell/Ada/Ampere)",
      "hardware": {
        "cuda_versions": ["12.4", "12.5", "12.6"],
        "compute_capability": ["8.6", "8.9", "9.0", "10.0"],
        "gpu_examples": ["RTX 5090", "RTX 5080", "RTX 5070", "RTX 4090", "RTX 4080", "RTX 4070", "RTX 3090", "RTX 3080", "RTX 3070", "RTX 3060"]
      },
      "packages": {
        "typing-extensions": "4.12.2",
        "numpy": "1.26.4",
        "torch": "2.5.1+cu124",
        "torchvision": "0.20.1+cu124",
        "torchaudio": "2.5.1+cu124",
        "transformers": ">=4.51.0,<4.60.0",
        "tokenizers": ">=0.21.0,<0.24.0",
        "safetensors": ">=0.7.0,<0.8.0",
        "huggingface-hub": ">=0.27.0,<0.28.0",
        "peft": ">=0.13.0,<0.16.0",
        "accelerate": ">=1.2.0,<1.3.0",
        "bitsandbytes": ">=0.45.0,<0.50.0",
        "datasets": ">=3.2.0,<4.0.0",
        "PySide6": "6.8.1",
        "PySide6-Essentials": "6.8.1",
        "PySide6-Addons": "6.8.1"
      },
      "tested": true,
      "notes": "Verified on RTX 4090 + RTX A2000",
      "binary_packages": {
        "triton": {
          "type": "wheel",
          "python": "cp312",
          "url": "https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post9/triton-3.2.0-cp312-cp312-win_amd64.whl"
        },
        "causal_conv1d": {
          "type": "wheel",
          "python": "cp312",
          "url": "https://github.com/d8ahazard/AudioLab/releases/download/1.0.0/causal_conv1d-1.5.0.post8-cp312-cp312-win_amd64.whl"
        },
        "mamba_ssm": {
          "type": "wheel",
          "python": "cp312",
          "url": "https://github.com/d8ahazard/AudioLab/releases/download/1.0.0/mamba_ssm-2.2.4-cp312-cp312-win_amd64.whl"
        }
      }
    },
    
    "cuda121_ampere": {
      "description": "CUDA 12.1-12.3, RTX 3000 series + A-series workstation",
      "hardware": {
        "cuda_versions": ["12.1", "12.2", "12.3"],
        "compute_capability": ["8.0", "8.6"],
        "gpu_examples": ["RTX 3090", "RTX 3080", "RTX 3070", "RTX 3060", "A100", "A30", "A2000", "A6000"]
      },
      "packages": {
        "typing-extensions": "4.12.2",
        "numpy": "1.26.4",
        "torch": "2.5.1+cu121",
        "torchvision": "0.20.1+cu121",
        "torchaudio": "2.5.1+cu121",
        "transformers": ">=4.51.0,<4.60.0",
        "tokenizers": ">=0.21.0,<0.24.0",
        "safetensors": ">=0.7.0,<0.8.0",
        "huggingface-hub": ">=0.27.0,<0.28.0",
        "peft": ">=0.13.0,<0.16.0",
        "accelerate": ">=1.2.0,<1.3.0",
        "bitsandbytes": ">=0.45.0,<0.50.0",
        "datasets": ">=3.2.0,<4.0.0",
        "PySide6": "6.8.1",
        "PySide6-Essentials": "6.8.1",
        "PySide6-Addons": "6.8.1"
      },
      "tested": true,
      "notes": "Verified on RTX A2000",
      "binary_packages": {
        "triton": {
          "type": "wheel",
          "python": "cp312",
          "url": "https://github.com/woct0rdho/triton-windows/releases/download/v3.2.0-windows.post9/triton-3.2.0-cp312-cp312-win_amd64.whl"
        },
        "causal_conv1d": {
          "type": "wheel",
          "python": "cp312",
          "url": "https://github.com/d8ahazard/AudioLab/releases/download/1.0.0/causal_conv1d-1.5.0.post8-cp312-cp312-win_amd64.whl"
        },
        "mamba_ssm": {
          "type": "wheel",
          "python": "cp312",
          "url": "https://github.com/d8ahazard/AudioLab/releases/download/1.0.0/mamba_ssm-2.2.4-cp312-cp312-win_amd64.whl"
        }
      }
    },
    
    "cuda118_turing": {
      "description": "CUDA 11.8, RTX 2000 series + Quadro T-series",
      "hardware": {
        "cuda_versions": ["11.8"],
        "compute_capability": ["7.5"],
        "gpu_examples": ["RTX 2080 Ti", "RTX 2080", "RTX 2070", "RTX 2060", "T1000", "T600", "T400"]
      },
      "packages": {
        "typing-extensions": "4.8.0",
        "numpy": "1.26.4",
        "torch": "2.5.1+cu118",
        "torchvision": "0.20.1+cu118",
        "torchaudio": "2.5.1+cu118",
        "transformers": ">=4.45.0,<4.60.0",
        "tokenizers": ">=0.21.0,<0.24.0",
        "safetensors": ">=0.4.0,<0.8.0",
        "huggingface-hub": ">=0.25.0,<0.28.0",
        "peft": ">=0.13.0,<0.16.0",
        "accelerate": ">=1.0.0,<1.3.0",
        "bitsandbytes": ">=0.45.0,<0.50.0",
        "datasets": ">=3.0.0,<4.0.0",
        "PySide6": "6.7.3",
        "PySide6-Essentials": "6.7.3",
        "PySide6-Addons": "6.7.3"
      },
      "tested": "in_progress",
      "notes": "Testing on T1000"
    }
  },
  
  "fallback_rules": {
    "unknown_newer_gpu": {
      "action": "use_profile",
      "profile": "cuda124_ampere_ada_blackwell",
      "reason": "Newer GPU - try latest CUDA profile"
    },
    "unknown_older_gpu": {
      "action": "use_profile",
      "profile": "cuda118_turing",
      "reason": "Older GPU - use conservative CUDA 11.8"
    },
    "no_cuda": {
      "action": "error",
      "message": "CPU-only mode not yet implemented. NVIDIA GPU required."
    },
    "low_vram": {
      "threshold_gb": 8,
      "action": "warn",
      "message": "GPU has less than 8GB VRAM. Fine-tuning large models may fail or require smaller batch sizes."
    },
    "very_low_vram": {
      "threshold_gb": 4,
      "action": "block",
      "message": "GPU has less than 4GB VRAM. This is insufficient for LLM fine-tuning. Minimum 6GB recommended."
    },
    "old_cuda": {
      "cuda_max": "11.7",
      "action": "warn",
      "message": "CUDA version is older than 11.8. Please update NVIDIA drivers for better compatibility."
    },
    "mixed_gpus": {
      "action": "use_lowest_common",
      "message": "Multiple GPUs detected with different capabilities. Using configuration for the least capable GPU to ensure compatibility."
    }
  },
  
  "compute_capability_map": {
    "10.0": {"architecture": "Blackwell", "profile": "cuda124_ampere_ada_blackwell"},
    "9.0": {"architecture": "Hopper", "profile": "cuda124_ampere_ada_blackwell"},
    "8.9": {"architecture": "Ada Lovelace", "profile": "cuda124_ampere_ada_blackwell"},
    "8.6": {"architecture": "Ampere (RTX 30xx)", "profile": "cuda121_ampere"},
    "8.0": {"architecture": "Ampere (A-series)", "profile": "cuda121_ampere"},
    "7.5": {"architecture": "Turing (RTX 20xx)", "profile": "cuda118_turing"}
  },
  
  "common_packages": {
    "sympy": "1.13.1",
    "jinja2": "3.1.6",
    "fsspec": "2025.9.0",
    "filelock": "3.20.1",
    "mpmath": "1.3.0",
    "regex": "2024.11.6",
    "pyyaml": "6.0.2",
    "requests": "2.32.3",
    "packaging": "24.2",
    "tqdm": "4.67.1",
    "sentencepiece": "0.2.0",
    "evaluate": "0.4.3",
    "einops": "0.8.1",
    "timm": "1.0.12",
    "open-clip-torch": "2.29.0",
    "pandas": "2.2.3",
    "psutil": "7.2.0",
    "streamlit": "1.41.1"
  },
  
  "blacklist": {
    "packages": ["torchao", "pytorch-ao", "torchao-nightly", "torch-ao"],
    "reason": "Cause transformers import failures"
  },
  
  "unsupported": {
    "gtx_10xx_pascal": "GTX 1080/1070/1060 (compute 6.1) support being dropped in PyTorch 2.8+. May work with CUDA 11.8 profile but untested.",
    "gtx_9xx_maxwell": "GTX 980/970 (compute 5.x) not supported by PyTorch 2.5+",
    "amd_gpu": "AMD GPUs (ROCm) not supported. This installer is NVIDIA-only.",
    "cpu_only": "CPU-only mode not implemented yet."
  }
}
